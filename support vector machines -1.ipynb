{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc88894-c701-4eab-8b5b-df99a810d837",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc388ce0-d3c3-4071-bed1-f55ecc7c269c",
   "metadata": {},
   "source": [
    "\n",
    "The polynomial kernel is a type of kernel function that is commonly used in machine learning algorithms, such as support vector machines (SVMs). It is a nonlinear function that takes two vectors as input and returns a value that measures the similarity between them. The polynomial kernel is defined as follows:\n",
    "\n",
    "K(x, y) = (x · y + c)^d\n",
    "where:\n",
    "\n",
    "x and y are the two vectors\n",
    "c is a constant term\n",
    "d is the degree of the polynomial\n",
    "The polynomial kernel can be used to transform the input data into a higher-dimensional space, where the relationships between the data points are more linear. This allows SVMs to learn non-linear models, even when the data is not linearly separable in the original space.\n",
    "\n",
    "The polynomial kernel is a flexible function that can be used to represent a variety of relationships between data points. The degree of the polynomial can be adjusted to control the complexity of the model. A higher degree will result in a more complex model that can fit the data more closely, but it may also be more prone to overfitting.\n",
    "\n",
    "In general, the polynomial kernel is a good choice for problems where the data is not linearly separable in the original space, but it is not too noisy. It is also a relatively efficient kernel, which makes it a good choice for large datasets.\n",
    "\n",
    "The relationship between polynomial functions and kernel functions is that the polynomial kernel is a special type of kernel function that is based on polynomial functions. The polynomial kernel takes two vectors as input and returns a value that is equal to the dot product of the vectors raised to a power. This is a special case of a kernel function, which is a function that takes two vectors as input and returns a value that measures the similarity between them.\n",
    "\n",
    "Kernel functions are used in a variety of machine learning algorithms, including SVMs, decision trees, and neural networks. They allow these algorithms to learn non-linear relationships between data points, even when the data is not linearly separable in the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985cefa-3411-4127-a547-086161b93580",
   "metadata": {},
   "source": [
    " Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17cef61-af5b-4f53-a5ee-050dd20003e6",
   "metadata": {},
   "source": [
    "\n",
    "The objective function of a linear SVM is to find a hyperplane that maximizes the margin between the two classes of data points. The margin is the distance between the hyperplane and the closest data points on either side. The objective function is defined as follows:\n",
    "\n",
    "min_w 1/2 * ||w||^2 + C * sum(max(0, 1 - yi * w^T xi))\n",
    "where:\n",
    "\n",
    "w is the weight vector\n",
    "C is a hyperparameter that controls the trade-off between minimizing the training error and minimizing the norm of the weight vector\n",
    "xi is the ith training example\n",
    "yi is the label for the ith training example\n",
    "The first term in the objective function is the squared norm of the weight vector. This term penalizes the weight vector for being large, which helps to prevent overfitting. The second term in the objective function is the sum of the slack variables. The slack variables are used to allow some of the training examples to be on the wrong side of the margin. The hyperparameter C controls how much the objective function penalizes misclassified training examples. A higher value of C will result in a more accurate model, but it may also be more prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb96b8-d762-4050-839e-3b8f53cc5f50",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b37eb-a38d-4f25-b5fb-c8db8641a66a",
   "metadata": {},
   "source": [
    "The kernel trick is a technique used in support vector machines (SVMs) to transform the data into a higher-dimensional space, where the data is linearly separable. This allows SVMs to learn non-linear models, even when the data is not linearly separable in the original space.\n",
    "\n",
    "The kernel trick works by replacing the dot product of two vectors in the original space with a kernel function. The kernel function is a mathematical function that measures the similarity between two vectors. The most common kernel function used in SVMs is the radial basis function (RBF) kernel.\n",
    "\n",
    "The RBF kernel is defined as follows:\n",
    "\n",
    "K(x, y) = exp(-||x - y||^2 / (2 * σ^2))\n",
    "where:\n",
    "\n",
    "x and y are two vectors\n",
    "σ is a hyperparameter that controls the width of the kernel\n",
    "The RBF kernel measures the similarity between two vectors by calculating the squared Euclidean distance between them and then exponentiating the result. The hyperparameter σ controls the width of the kernel. A smaller value of σ will result in a narrower kernel, which will only match vectors that are very similar. A larger value of σ will result in a wider kernel, which will match vectors that are less similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923dda4f-000a-46f9-a027-7a517fcc0573",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedcaf08-8673-43a3-95b5-1ec2cd564bc3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Support vectors are the data points that are closest to the hyperplane in a support vector machine (SVM). They are the ones that the SVM algorithm pays the most attention to when it is trying to find the hyperplane that maximizes the margin between the two classes of data points.\n",
    "\n",
    "Here is an analogy that might help you understand the role of support vectors. Imagine you are trying to build a fence to keep two herds of animals separate. You want to build the fence in a way that maximizes the distance between the two herds. You would probably start by placing the fence posts at the points where the two herds are closest together. These would be the support vectors. The rest of the fence posts would then be placed in between the support vectors, in a way that keeps the two herds separate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba55eb-84ea-4ed7-b188-4d35885dd241",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9702d-519a-4427-8264-c7a347787c72",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) are a type of machine learning algorithm used for classification and regression tasks. SVM aims to find a hyperplane that best separates data points of different classes in a high-dimensional space \n",
    "\n",
    "1. Hyperplane:\n",
    "\n",
    "A hyperplane in an n-dimensional space is a flat affine subspace of dimension n-1. In the context of SVM, for a two-class classification problem, the hyperplane is a decision boundary that separates the data points of one class from the other.\n",
    "Example: Consider a simple 2D dataset with two classes, represented by red circles and blue squares. The hyperplane (represented by the black line) separates the two classes. \n",
    "\n",
    "2. Marginal Plane:\n",
    "\n",
    "The marginal plane in SVM refers to the hyperplane that is equidistant from the support vectors of both classes. Support vectors are the data points that are closest to the hyperplane and influence its position.\n",
    "Example: In the graph below, the marginal plane (represented by the dashed line) is equidistant from the support vectors of both classes. The support vectors are shown with larger markers. \n",
    "\n",
    "3. Hard Margin:\n",
    "\n",
    "In a hard margin SVM, the algorithm seeks to find a hyperplane that perfectly separates the two classes without allowing any misclassification. This approach works well when the data is linearly separable, but it can be sensitive to outliers.\n",
    "Example: The graph below shows a hard margin SVM. The solid black line perfectly separates the two classes without any misclassification. \n",
    "\n",
    "4. Soft Margin:\n",
    "\n",
    "In a soft margin SVM, the algorithm allows for a certain degree of misclassification to find a more flexible hyperplane that can handle some outliers. The objective is to balance between maximizing the margin and minimizing misclassification.\n",
    "Example: In the graph below, the soft margin SVM allows for some misclassification by introducing a margin that is not as wide as the hard margin SVM. This flexibility can help handle outliers and achieve better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49d4a9-07bc-4e2d-92a9-8de52f00c35f",
   "metadata": {},
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "- Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "-  Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "-  Compute the accuracy of the model on the testing setl\n",
    "-  Plot the decision boundaries of the trained model using two of the featuresl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a14c955a-a815-4480-9f0d-5b052a99c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b4ac95-292a-4f3b-bae5-4ec26be5ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,:2]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beacf070-ee71-4d99-bb57-8904ce42683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into a traning set and a testing set\n",
    "X_train, X_test, y_train,y_test = train_test_split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
